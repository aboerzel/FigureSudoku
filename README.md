# ğŸ§© FigureSudoku RL-Agent

Dieses Projekt demonstriert den Einsatz von **Reinforcement Learning** (BestÃ¤rkendes Lernen), um eine komplexe Sudoku-Variante zu lÃ¶sen. Anstelle von Zahlen verwendet dieses Sudoku geometrische **Formen** und **Farben**, was die logischen Anforderungen an den Agenten erhÃ¶ht.

---

## ğŸ¨ Das Spielkonzept

Das **FigureSudoku** basiert auf einem 4x4-Gitter. Jedes Feld muss eine eindeutige Kombination aus einer Form und einer Farbe enthalten.

### Die Attribute:
*   **Geometrien:** ğŸ”µ Kreis, ğŸŸ¥ Quadrat, â–² Dreieck, â¬¢ Hexagon
*   **Farben:** â¤ï¸ Rot, ğŸ’š GrÃ¼n, ğŸ’™ Blau, ğŸ’› Gelb

### Die Regeln:
1.  Jedes Feld muss eine Figur (Form + Farbe) enthalten.
2.  In jeder **Reihe** und jeder **Spalte** darf jede Form nur einmal vorkommen.
3.  In jeder **Reihe** und jeder **Spalte** darf jede Farbe nur einmal vorkommen.
4.  Jede Kombination (z.B. "Roter Kreis") darf im gesamten Gitter nur einmal existieren.

---

## ğŸš€ Die KI-Architektur

Der Agent nutzt modernste Deep-Learning-Techniken, um die Spielregeln von Grund auf zu lernen:

*   **Algorithmus:** `MaskablePPO` (Proximal Policy Optimization). Dank **Action Masking** lernt der Agent keine ungÃ¼ltigen ZÃ¼ge, was das Training massiv beschleunigt.
*   **Neuronales Netz:** Ein **CNN (Convolutional Neural Network)** mit **Residual Blocks (ResNet)**. Dies erlaubt der KI, rÃ¤umliche ZusammenhÃ¤nge zwischen Reihen und Spalten wie ein menschliches Auge zu erfassen.
*   **Curriculum Learning:** Das Training startet bei Level 1 (fast gelÃ¶st) und steigert automatisch den Schwierigkeitsgrad bis Level 10 (viele leere Felder), sobald der Agent eine Erfolgsquote von 98% erreicht.
*   **Observation Space:** Ein 3D-Tensor (10 KanÃ¤le), der One-Hot-kodiert die Positionen aller Formen und Farben reprÃ¤sentiert.

---

## ğŸ“‚ Projektstruktur

```text
FigureSudoku/
â”œâ”€â”€ ğŸ“„ config.py             # Zentrale Konfiguration (Hyperparameter, Level, etc.)
â”œâ”€â”€ ğŸ“„ train.py              # Hauptskript zum Starten des KI-Trainings
â”œâ”€â”€ ğŸ“„ figure_sudoko_env.py  # Die Gymnasium-Umgebung (Logik & Rewards)
â”œâ”€â”€ ğŸ“„ sudoku_generator.py   # Backtracking-Algorithmus zur RÃ¤tsel-Generierung (mit optionaler EindeutigkeitsprÃ¼fung)
â”œâ”€â”€ ğŸ“„ sudoku_game.py        # Grafische OberflÃ¤che zum Spielen & Evaluieren
â”œâ”€â”€ ğŸ“„ visualizer.py         # Live-Visualisierung wÃ¤hrend des Trainings
â”œâ”€â”€ ğŸ“„ callbacks.py          # Logik fÃ¼r Curriculum Learning & Modell-Speicherung
â”œâ”€â”€ ğŸ“„ shapes.py             # Definitionen der Formen und Farben (Enums)
â””â”€â”€ ğŸ“ output/               # Gespeicherte Modelle, Logs und Checkpoints
```

---

## ğŸ›  Setup & Installation

### Voraussetzungen:
*   Python 3.8+
*   Anaconda oder venv (empfohlen)

### Installation der AbhÃ¤ngigkeiten:
```bash
pip install torch stable-baselines3 sb3-contrib gym==0.21.0 numpy
```

---

## ğŸ‹ï¸ Training starten

Um den Agenten zu trainieren, fÃ¼hre einfach die `train.py` aus. Die Konfiguration kann in der `config.py` angepasst werden (z.B. `NUM_AGENTS` fÃ¼r Parallelisierung).

```bash
python train.py
```

### Monitoring mit TensorBoard:
WÃ¤hrend das Training lÃ¤uft, kannst du den Fortschritt (Erfolgsquote, Reward) live verfolgen:
```bash
tensorboard --logdir output/SUDOKU/logs/train --port 6006
```
Ã–ffne dann `http://localhost:6006` in deinem Browser.

---

## ğŸ® Den Agenten beobachten (Test/Demo)

Wenn du sehen mÃ¶chtest, wie die trainierte KI ein RÃ¤tsel lÃ¶st, kannst du die GUI nutzen:

1.  Stelle sicher, dass ein trainiertes Modell im `output`-Ordner liegt.
2.  Starte das Spiel:
```bash
python sudoku_game.py --level 10
```
3.  Klicke auf **"New Game"** und dann auf **"Solve"**, um den Agenten beim LÃ¶sen zuzusehen.

---

## ğŸ“Š Visualisierung des Trainings
Wenn in der `config.py` der Parameter `RENDER_GUI = True` gesetzt ist, Ã¶ffnet das Training fÃ¼r jeden Agenten ein eigenes Fenster. So kannst du live beobachten, wie die KI verschiedene Strategien ausprobiert.

---
*Entwickelt als Experimentierfeld fÃ¼r Reinforcement Learning in komplexen Constraint-Umgebungen.*
