# ğŸ§© Figure-Sudoku RL-Agent

<img src="./documentation/screenshot.png" width="500" alt="Figure-Sudoku Screenshot">

Dieses Projekt demonstriert den Einsatz von **Reinforcement Learning** (BestÃ¤rkendes Lernen), um eine komplexe Sudoku-Variante zu lÃ¶sen. Anstelle von Zahlen verwendet dieses Sudoku geometrische **Formen** und **Farben**, was die logischen Anforderungen an den Agenten erhÃ¶ht.

---

## ğŸ¨ Das Spielkonzept

Das **Figure-Sudoku** basiert auf einem 4x4-Gitter. Jedes Feld muss eine eindeutige Kombination aus einer Form und einer Farbe enthalten.

### Die Attribute:
*   **Geometrien:** ğŸ”µ Kreis, ğŸŸ¥ Quadrat, â–² Dreieck, â¬¢ Hexagon
*   **Farben:** â¤ï¸ Rot, ğŸ’š GrÃ¼n, ğŸ’™ Blau, ğŸ’› Gelb

### Die Regeln:
1.  Jedes Feld muss eine Figur (Form + Farbe) enthalten.
2.  In jeder **Reihe** und jeder **Spalte** darf jede Form nur einmal vorkommen.
3.  In jeder **Reihe** und jeder **Spalte** darf jede Farbe nur einmal vorkommen.
4.  Jede Kombination (z.B. "Roter Kreis") darf im gesamten Gitter nur einmal existieren.
5.  **Teilvorgaben:** Es ist mÃ¶glich, dass Felder nur mit einer Form (ohne Farbe) oder nur mit einer Farbe (ohne Form) vorbelegt sind. Der Agent muss dann die jeweils fehlende Komponente logisch korrekt ergÃ¤nzen.

---

## ğŸš€ Die KI-Architektur

Der Agent nutzt modernste Deep-Learning-Techniken, um die Spielregeln von Grund auf zu lernen:

*   **Bibliotheken:** Nutzt **Stable Baselines3 (v2.0+)** und **Gymnasium**, die aktuelle Standard-Schnittstelle fÃ¼r Reinforcement Learning.
*   **Algorithmus:** `MaskablePPO` (Proximal Policy Optimization). Dank **Action Masking** lernt der Agent keine ungÃ¼ltigen ZÃ¼ge, was das Training massiv beschleunigt.
*   **CNN (Convolutional Neural Network) mit Residual Blocks (ResNet):** Da Sudoku-Regeln auf rÃ¤umlichen AbhÃ¤ngigkeiten (Zeilen/Spalten) basieren, nutzt der Agent Faltungsschichten. ResNet-BlÃ¶cke helfen dabei, auch tieferliegende AbhÃ¤ngigkeiten ohne Informationsverlust zu lernen.
*   **Observation Space:** Ein 3D-Tensor (10 KanÃ¤le), der One-Hot-kodiert die Positionen aller Formen und Farben reprÃ¤sentiert (flattened auf 160 EingÃ¤nge).
*   **Action Space:** Insgesamt 256 diskrete Aktionen. Jede Aktion entspricht der Kombination aus einer bestimmten Figur (16 MÃ¶glichkeiten) und einem Zielfeld (16 Felder).
*   **Action Masking:** Da in jedem Zustand nur wenige der 256 Aktionen regelkonform sind, nutzt das Projekt **Action Masking**. Dies verhindert, dass der Agent ungÃ¼ltige ZÃ¼ge (z.B. doppelte Farbe in einer Reihe) Ã¼berhaupt in ErwÃ¤gung zieht. Dies reduziert den Suchraum dramatisch und stabilisiert das Training (siehe Abschnitt [Action Masking](#-action-masking-detailerklÃ¤rung)).
*   **Curriculum Learning:** Das Training startet bei Level 1 (fast gelÃ¶st) und steigert automatisch den Schwierigkeitsgrad bis Level 12 (viele leere Felder), sobald der Agent eine definierte Erfolgsquote (einstellbar Ã¼ber `REWARD_THRESHOLD`) erreicht.
*   **Fortsetzbarkeit:** Das Training erkennt automatisch vorhandene Modelle. Das Start-Level wird primÃ¤r Ã¼ber `START_LEVEL` in der `config.py` gesteuert. Ist dieser Wert auf `None` gesetzt, wird das Level automatisch aus dem letzten Log-Eintrag (`LOG_FILE_PATH`) ermittelt (mit Fallback auf Level 1).
*   **RÃ¤tsel-Generator:** Die RÃ¤tsel werden mithilfe eines hochoptimierten Backtracking-Algorithmus generiert (`sudoku_generator.py`). Dieser nutzt die **HCDS-Metrik** (Human-Centric Difficulty System), um gezielt Schwierigkeitsgrade von Level 1 bis 12 zu erzeugen, die das menschliche Schwierigkeitsempfinden abbilden. Er stellt sicher, dass jede Aufgabe eine eindeutige LÃ¶sung besitzt. Ab Level 11 werden zudem Teilvorgaben (Partial Shapes) unterstÃ¼tzt.

---

## ğŸ§  Funktionsweise des Agenten

Der LÃ¶sungsprozess folgt einem klassischen RL-Zyklus:

1.  **Beobachtung:** Der Agent sieht das aktuelle 4x4-Gitter als One-Hot-Vektor.
2.  **Maskierung:** Die Umgebung berechnet alle regelkonformen ZÃ¼ge basierend auf den Sudoku-Regeln.
3.  **Entscheidung:** Das neuronale Netz bewertet die validen Aktionen und wÃ¤hlt die Erfolgversprechendste aus.
4.  **Belohnung:** FÃ¼r jeden korrekten Zug erhÃ¤lt der Agent einen kleinen Reward. Das LÃ¶sen des gesamten RÃ¤tsels gibt einen groÃŸen Bonus.
5.  **Lernen:** Ãœber PPO optimiert der Agent seine Strategie, um die kumulierte Belohnung zu maximieren.

---

## ğŸ§© RÃ¤tsel-Generator & Schwierigkeit (HCDS)

Der neue Generator (`sudoku_generator.py`) basiert auf dem **Human-Centric Difficulty System (HCDS)**. Er wurde speziell optimiert, um das menschliche Schwierigkeitsempfinden in den Level-Stufen 1-12 abzubilden und auch bei hohen Schwierigkeitsgraden eine schnelle RÃ¤tsel-Generierung (ca. 3s fÃ¼r Level 12) bei garantierter Eindeutigkeit zu ermÃ¶glichen.

### Schwierigkeitsstufen (1-12):
*   **Level 1-10:** Die Schwierigkeit skaliert linear durch das Entfernen von Feldern, bis der Ziel-HCDS-Wert erreicht ist.
*   **Level 11:** EnthÃ¤lt zusÃ¤tzlich **eine Teilvorgabe** (nur Form oder nur Farbe).
*   **Level 12:** EnthÃ¤lt **zwei Teilvorgaben**.

### Performance-Features:
*   **MRV-Heuristik (Minimum Remaining Values):** Beschleunigt die EindeutigkeitsprÃ¼fung durch intelligente Wahl des nÃ¤chsten Feldes im Backtracking.
*   **In-Place Backtracking:** Minimiert Speicherallokationen und CPU-Last.
*   **Inkrementelle HCDS-Berechnung:** Effiziente Bewertung der Schwierigkeit wÃ¤hrend des Generierungsprozesses.

---

## ğŸ›¡ï¸ Action Masking (DetailerklÃ¤rung)

Action Masking ist eine entscheidende Technik fÃ¼r die Effizienz dieses Agenten. Da der Action Space mit **256 Aktionen** sehr groÃŸ ist, aber in jedem Spielzustand oft nur **weniger als 5%** der ZÃ¼ge legal sind, wÃ¼rde ein Standard-RL-Agent extrem lange brauchen, um allein die Grundregeln (z.B. "nicht zweimal Rot in eine Spalte") durch reines Ausprobieren (*Trial & Error*) zu lernen.

### Wie es funktioniert:
Bevor der Agent eine Aktion auswÃ¤hlt, berechnet die Umgebung (`FigureSudokuEnv.action_masks()`) einen binÃ¤ren Vektor (die Maske). FÃ¼r jede der 256 Aktionen wird geprÃ¼ft:

1.  **Feldbelegung:** Ist das Zielfeld bereits mit einer anderen Figur belegt? (Oder passt die gewÃ¤hlte Figur zu einer bestehenden Teilvorgabe?)
2.  **Figur-VerfÃ¼gbarkeit:** Wurde die Kombination aus Form und Farbe (z.B. "Blauer Kreis") bereits an einer anderen Stelle im Gitter platziert?
3.  **Sudoku-Constraints (Reihe/Spalte):** Existiert die gewÃ¤hlte Form oder die gewÃ¤hlte Farbe bereits in der Ziel-Reihe oder Ziel-Spalte?

### Warum MaskablePPO?
In einem Standard-PPO-Algorithmus wÃ¼rde der Agent auch ungÃ¼ltige Aktionen wÃ¤hlen, eine negative Belohnung erhalten und dann mÃ¼hsam lernen, diese Aktionen zu vermeiden.
**MaskablePPO** hingegen nutzt die Maske direkt in der Wahrscheinlichkeitsverteilung der Policy:
*   UngÃ¼ltige Aktionen erhalten eine Wahrscheinlichkeit von **exakt Null**.
*   Der Agent "sieht" wÃ¤hrend der Entscheidungsfindung nur die legalen Optionen.
*   **Vorteil:** Das neuronale Netz muss keine KapazitÃ¤t darauf verschwenden, die harten Regeln des Spiels auswendig zu lernen, sondern kann sich sofort auf die **LÃ¶sungsstrategie** konzentrieren.

---

### ğŸ“‚ Projektstruktur

```text
FigureSudoku/
â”œâ”€â”€ ğŸ“„ config.py             # Zentrale Konfiguration (Hyperparameter, Level, etc.)
â”œâ”€â”€ ğŸ“„ train.py              # Hauptskript zum Starten des KI-Trainings
â”œâ”€â”€ ğŸ“„ figure_sudoku_env.py  # Die Gymnasium-Umgebung (Logik & Rewards)
â”œâ”€â”€ ğŸ“„ sudoku_generator.py   # Hochoptimierter Generator mit HCDS-Metrik & EindeutigkeitsprÃ¼fung
â”œâ”€â”€ ğŸ“„ sudoku_game.py        # Grafische Desktop-OberflÃ¤che (Tkinter)
â”œâ”€â”€ ğŸ“„ streamlit_app.py      # Moderne Web-Applikation (Streamlit)
â”œâ”€â”€ ğŸ“„ visualizer.py         # Live-Visualisierung wÃ¤hrend des Trainings
â”œâ”€â”€ ğŸ“„ callbacks.py          # Logik fÃ¼r Curriculum Learning & Modell-Speicherung
â”œâ”€â”€ ğŸ“„ shapes.py             # Definitionen der Formen und Farben (Enums)
â”œâ”€â”€ ğŸ“ documentation/        # Projekt-Dokumentation & Videos
â””â”€â”€ ğŸ“ output/               # Gespeicherte Modelle, Logs und Checkpoints
```

---

## ğŸ¬ Demo

Hier siehst du den RL-Agenten in Aktion, wie er ein Figure-Sudoku schrittweise lÃ¶st:

<div align="center">
  <video src="./documentation/solving_sudoku_game.mp4" width="600" controls autoplay loop muted>
    Ihr Browser unterstÃ¼tzt das Video-Tag nicht.
  </video>
  <p><i>Agent beim LÃ¶sen eines Figure-Sudokus (RL MaskablePPO)</i></p>
</div>

> **Hinweis:** Falls das Video oben nicht automatisch startet, kannst du es direkt hier ansehen: [Demo-Video Ã¶ffnen](./documentation/solving_sudoku_game.mp4)

---

## âš™ï¸ Konfiguration (`config.py`)

Die zentralen Einstellungen des Projekts werden in der `config.py` vorgenommen. Hier eine Ãœbersicht der wichtigsten Parameter:

### ğŸ§© Generator (RÃ¤tsel-Erstellung)
*   `START_LEVEL`: Bestimmt das Start-Level fÃ¼r das Training. Wenn ein Wert (1-12) angegeben ist, wird dieser fest verwendet (manuelles Ãœberschreiben). Ist `None` gesetzt, wird das Level beim Fortsetzen eines Trainings automatisch aus der Log-Datei ermittelt (Fallback: Level 1). [Bereich: `1` bis `12` oder `None`]
*   `MAX_LEVEL`: Das Ziel-Level (hÃ¶chste Schwierigkeit). [Bereich: `1` bis `12`]

### âš¡ Training & Hyperparameter
*   `NUM_AGENTS`: Anzahl der parallelen Trainings-Umgebungen. [Bereich: `>= 1`]
*   `REWARD_THRESHOLD`: Die benÃ¶tigte Erfolgsquote (z.B. `0.90` fÃ¼r 90%), um in das nÃ¤chste Level aufzusteigen. [Bereich: `0.0` bis `1.0`]
*   `CHECK_FREQ`: Intervall (in Schritten), in dem die Erfolgsquote geprÃ¼ft und Modelle zwischengespeichert werden. [Bereich: `>= 1`]
*   `TOTAL_TIMESTEPS`: Die Gesamtdauer des Trainings (Gesamtzahl der Schritte Ã¼ber alle Agenten). [Bereich: `>= 1`]
*   `MAX_TIMESTEPS`: Maximale Anzahl an Schritten pro Episode. Verhindert Endlosschleifen bei unlÃ¶sbaren ZustÃ¤nden.

### ğŸ† Belohnungssystem (Rewards)

Das Belohnungssystem ist darauf ausgelegt, den Agenten zu einem effizienten und regelkonformen LÃ¶sungsweg zu fÃ¼hren. Es besteht aus drei Hauptkomponenten:

1.  **`REWARD_SOLVED` (Aktuell: `10.0`)**:
    *   **Zweck:** Der "Heilige Gral". Dies ist die maximale Belohnung, die der Agent erhÃ¤lt, wenn das gesamte Gitter regelkonform gefÃ¼llt ist.
    *   **Warum dieser Wert?** Er muss deutlich hÃ¶her sein als die Summe der EinzelzÃ¼ge, damit der Agent das Ã¼bergeordnete Ziel (das LÃ¶sen) priorisiert. Selbst auf dem hÃ¶chsten Schwierigkeitsgrad (Level 12) betrÃ¤gt die Summe aller validen Einzelzug-Belohnungen nur ca. `2.45`, was bedeutet, dass der `REWARD_SOLVED` (10.0) immer noch mehr als das Vierfache davon wert ist. Dies stellt sicher, dass der Agent auch bei komplexen RÃ¤tseln stets motiviert bleibt, das RÃ¤tsel vollstÃ¤ndig zu lÃ¶sen.

2.  **`REWARD_VALID_MOVE_BASE` (Aktuell: `0.1`)**:
    *   **Zweck:** Belohnung fÃ¼r jeden korrekten Zug.
    *   **Dynamische Skalierung:** Die tatsÃ¤chliche Belohnung berechnet sich als: `base * (1 + leere_felder / gitter_grÃ¶ÃŸe)`.
    *   **Warum diese Logik?** Durch die Skalierung erhÃ¤lt der Agent fÃ¼r ZÃ¼ge auf einem leeren Board (wo es viele MÃ¶glichkeiten gibt) eine hÃ¶here Belohnung als fÃ¼r ZÃ¼ge auf einem fast vollen Board. Dies motiviert den Agenten, "schwierige" Entscheidungen frÃ¼hzeitig korrekt zu treffen. Der Basiswert von `0.1` ist klein genug, um "Reward Shaping" zu ermÃ¶glichen, ohne das Endziel zu Ã¼berschatten.

3.  **`REWARD_INVALID_MOVE` (Aktuell: `-0.5`)**:
    *   **Zweck:** Bestrafung fÃ¼r illegale ZÃ¼ge (obwohl diese durch Action Masking weitgehend verhindert werden).
    *   **Warum dieser Wert?** Die Strafe ist moderat negativ gewÃ¤hlt. Da der Agent `MaskablePPO` nutzt, trifft er selten auf ungÃ¼ltige ZÃ¼ge im Action Space, aber die Strafe dient als zusÃ¤tzliche Absicherung fÃ¼r die LernstabilitÃ¤t der Policy.

---

## ğŸ–¼ï¸ Visualisierung (Training)

WÃ¤hrend des Trainings kann der Fortschritt auf zwei Arten visualisiert werden:

*   **Live-GUI:** Wenn `config.RENDER_GUI = True` gesetzt ist, wird der Spielzustand der Agenten live in einem Fenster (`visualizer.py`) angezeigt.
*   **TensorBoard:** Detaillierte Metriken (Reward, Erfolgsquote, Training-Loss) werden geloggt.

---

## ğŸ›  Setup & Installation

### Voraussetzungen:
*   Python 3.8+
*   Anaconda oder venv (empfohlen)
*   CUDA-fÃ¤hige GPU (fÃ¼r Training empfohlen, z.B. CUDA 11.8)

### Installation der AbhÃ¤ngigkeiten:

1.  **PyTorch mit CUDA-Support (Beispiel fÃ¼r CUDA 11.8):**
    ```bash
    pip install torch==2.3.1+cu118 torchvision==0.18.1+cu118 torchaudio==2.3.1+cu118 --extra-index-url https://download.pytorch.org/whl/cu118
    ```

2.  **Restliche Anforderungen:**
    ```bash
    pip install -r requirements.txt
    ```

---

## ğŸ‹ï¸ Training starten

Um den Agenten zu trainieren, fÃ¼hre einfach die `train.py` aus. Die Konfiguration kann in der `config.py` angepasst werden (z.B. `NUM_AGENTS` fÃ¼r Parallelisierung).

```bash
python train.py | Tee-Object -FilePath output/SUDOKU/training.log
```

### Monitoring mit TensorBoard:
WÃ¤hrend das Training lÃ¤uft, kannst du den Fortschritt (Erfolgsquote, Reward) live verfolgen. Der Pfad ist in `config.TENSORBOARD_TRAIN_LOG` definiert:
```bash
# Beispiel (standardmÃ¤ÃŸig):
tensorboard --logdir output/SUDOKU/logs/train --port 6006
```
Ã–ffne dann `http://localhost:6006` in deinem Browser.

---

## ğŸ® Spielen & Den Agenten beobachten

Es stehen zwei OberflÃ¤chen zur VerfÃ¼gung, um das Spiel selbst zu spielen oder die KI beim LÃ¶sen zu beobachten.

### ğŸŒ Web-Applikation (Streamlit) - Empfohlen
Eine moderne, interaktive WeboberflÃ¤che, die im Browser lÃ¤uft.
```bash
streamlit run streamlit_app.py
```

### ğŸ–¥ï¸ Desktop-Anwendung (Tkinter)
Die klassische Version mit Drag & Drop FunktionalitÃ¤t.
```bash
python sudoku_game.py
```

### Anleitung:
1.  Stelle sicher, dass ein trainiertes Modell im `output`-Ordner liegt (siehe `config.MODEL_PATH`).
2.  WÃ¤hle den Schwierigkeitsgrad Ã¼ber den **"Level"-Slider** aus.
3.  Klicke auf **"Neues Spiel"** (oder generiere ein neues RÃ¤tsel).
4.  Klicke auf **"LÃ¶sen"**, um den Agenten beim LÃ¶sen zuzusehen, oder spiele selbst!

---

## ğŸ“Š Visualisierung des Trainings (Live)
Wenn in der `config.py` der Parameter `RENDER_GUI = True` gesetzt ist, Ã¶ffnet das Training fÃ¼r jeden Agenten ein eigenes Fenster (`visualizer.py`). So kannst du live beobachten, wie die KI verschiedene Strategien ausprobiert.

---

## ğŸ“„ Lizenz & Autor

*   **Autor:** Andreas BÃ¶rzel
*   **GitHub:** [Figure-Sudoku](https://github.com/aboerzel/FigureSudoku)
*   **Lizenz:** [MIT License](LICENSE) (oder siehe Dateikopf)

*Entwickelt als Experimentierfeld fÃ¼r Reinforcement Learning in komplexen Constraint-Umgebungen.*
