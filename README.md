# üß© FigureSudoku RL-Agent

Dieses Projekt demonstriert den Einsatz von **Reinforcement Learning** (Best√§rkendes Lernen), um eine komplexe Sudoku-Variante zu l√∂sen. Anstelle von Zahlen verwendet dieses Sudoku geometrische **Formen** und **Farben**, was die logischen Anforderungen an den Agenten erh√∂ht.

---

## üé® Das Spielkonzept

Das **FigureSudoku** basiert auf einem 4x4-Gitter. Jedes Feld muss eine eindeutige Kombination aus einer Form und einer Farbe enthalten.

### Die Attribute:
*   **Geometrien:** üîµ Kreis, üü• Quadrat, ‚ñ≤ Dreieck, ‚¨¢ Hexagon
*   **Farben:** ‚ù§Ô∏è Rot, üíö Gr√ºn, üíô Blau, üíõ Gelb

### Die Regeln:
1.  Jedes Feld muss eine Figur (Form + Farbe) enthalten.
2.  In jeder **Reihe** und jeder **Spalte** darf jede Form nur einmal vorkommen.
3.  In jeder **Reihe** und jeder **Spalte** darf jede Farbe nur einmal vorkommen.
4.  Jede Kombination (z.B. "Roter Kreis") darf im gesamten Gitter nur einmal existieren.

---

## üöÄ Die KI-Architektur

Der Agent nutzt modernste Deep-Learning-Techniken, um die Spielregeln von Grund auf zu lernen:

*   **Algorithmus:** `MaskablePPO` (Proximal Policy Optimization). Dank **Action Masking** lernt der Agent keine ung√ºltigen Z√ºge, was das Training massiv beschleunigt.
*   **Neuronales Netz:** Ein **CNN (Convolutional Neural Network)** mit **Residual Blocks (ResNet)**. Dies erlaubt der KI, r√§umliche Zusammenh√§nge zwischen Reihen und Spalten wie ein menschliches Auge zu erfassen.
*   **Curriculum Learning:** Das Training startet bei Level 1 (fast gel√∂st) und steigert automatisch den Schwierigkeitsgrad bis Level 12 (viele leere Felder), sobald der Agent eine Erfolgsquote von √ºber 98% erreicht. Dies ist √ºber `REWARD_THRESHOLD` in der `config.py` einstellbar.
*   **Hyperparameter-Optimierung:** Einsatz von `target_kl` zur Stabilisierung der Policy-Updates und ein `linear_schedule` f√ºr die Lernrate, um ein sauberes Konvergieren zu erm√∂glichen.
*   **Observation Space:** Ein 3D-Tensor (10 Kan√§le), der One-Hot-kodiert die Positionen aller Formen und Farben repr√§sentiert (flattened auf 160 Eing√§nge f√ºr die Kompatibilit√§t).

---

## üìÇ Projektstruktur

```text
FigureSudoku/
‚îú‚îÄ‚îÄ üìÑ config.py             # Zentrale Konfiguration (Hyperparameter, Level, etc.)
‚îú‚îÄ‚îÄ üìÑ train.py              # Hauptskript zum Starten des KI-Trainings
‚îú‚îÄ‚îÄ üìÑ figure_sudoko_env.py  # Die Gymnasium-Umgebung (Logik & Rewards)
‚îú‚îÄ‚îÄ üìÑ sudoku_generator.py   # Backtracking-Algorithmus zur R√§tsel-Generierung (mit optionaler Eindeutigkeitspr√ºfung)
‚îú‚îÄ‚îÄ üìÑ sudoku_game.py        # Grafische Oberfl√§che zum Spielen & Evaluieren
‚îú‚îÄ‚îÄ üìÑ visualizer.py         # Live-Visualisierung w√§hrend des Trainings
‚îú‚îÄ‚îÄ üìÑ callbacks.py          # Logik f√ºr Curriculum Learning & Modell-Speicherung
‚îú‚îÄ‚îÄ üìÑ shapes.py             # Definitionen der Formen und Farben (Enums)
‚îî‚îÄ‚îÄ üìÅ output/               # Gespeicherte Modelle, Logs und Checkpoints
```

---

## ‚öôÔ∏è Konfiguration (`config.py`)

Die zentralen Einstellungen des Projekts werden in der `config.py` vorgenommen. Hier eine √úbersicht der wichtigsten Parameter:

### üß© Generator (R√§tsel-Erstellung)
*   `START_LEVEL`: Level, bei dem das Training beginnt (Anzahl leerer Felder). [Bereich: `1` bis `16`]
*   `MAX_LEVEL`: Das Ziel-Level (h√∂chste Schwierigkeit). [Bereich: `1` bis `16`, aktuell `12`]
*   `UNIQUE`: Stellt sicher, dass jedes generierte R√§tsel nur genau eine g√ºltige L√∂sung hat. [Werte: `True`, `False`]
*   `PARTIAL_PROB`: Wahrscheinlichkeit f√ºr das Auftreten von Feldern, bei denen nur die Form oder nur die Farbe vorgegeben ist. [Bereich: `0.0` bis `1.0`]
*   `PARTIAL_MODE`: Modus f√ºr die Teilvorgaben (`0`: Aus, `1`: genau 2 Felder, `2`: 1-2 Felder zuf√§llig). [Werte: `0`, `1`, `2`]

### ‚ö° Training & Hyperparameter
*   `NUM_AGENTS`: Anzahl der parallelen Trainings-Umgebungen. [Bereich: `>= 1`]
*   `REWARD_THRESHOLD`: Die ben√∂tigte Erfolgsquote (z.B. `0.90` f√ºr 90%), um in das n√§chste Level aufzusteigen. [Bereich: `0.0` bis `1.0`]
*   `CHECK_FREQ`: Intervall (in Schritten), in dem die Erfolgsquote gepr√ºft und Modelle zwischengespeichert werden. [Bereich: `>= 1`]
*   `TOTAL_TIMESTEPS`: Die Gesamtdauer des Trainings (Gesamtzahl der Schritte √ºber alle Agenten). [Bereich: `>= 1`]

### üèÜ Belohnungssystem (Rewards)
*   `REWARD_SOLVED`: Belohnung f√ºr ein komplett gel√∂stes Sudoku. [Typ: `Float`, empfohlen: `> 0`]
*   `REWARD_VALID_MOVE_BASE`: Kleine Belohnung f√ºr jeden korrekten Setzvorgang. [Typ: `Float`, empfohlen: `> 0`]
*   `REWARD_INVALID_MOVE`: Strafe f√ºr den Versuch, eine Figur entgegen der Regeln zu platzieren. [Typ: `Float`, empfohlen: `< 0`]

### üñºÔ∏è Visualisierung
*   `RENDER_GUI`: Aktiviert die Live-Anzeige der Agenten w√§hrend des Trainings. [Werte: `True`, `False`]

---

## üõ† Setup & Installation

### Voraussetzungen:
*   Python 3.8+
*   Anaconda oder venv (empfohlen)

### Installation der Abh√§ngigkeiten:
```bash
pip install torch stable-baselines3 sb3-contrib gym==0.21.0 numpy
```

---

## üèãÔ∏è Training starten

Um den Agenten zu trainieren, f√ºhre einfach die `train.py` aus. Die Konfiguration kann in der `config.py` angepasst werden (z.B. `NUM_AGENTS` f√ºr Parallelisierung).

```bash
python train.py
```

### Monitoring mit TensorBoard:
W√§hrend das Training l√§uft, kannst du den Fortschritt (Erfolgsquote, Reward) live verfolgen:
```bash
tensorboard --logdir output/SUDOKU/logs/train --port 6006
```
√ñffne dann `http://localhost:6006` in deinem Browser.

---

## üéÆ Den Agenten beobachten (Test/Demo)

Wenn du sehen m√∂chtest, wie die trainierte KI ein R√§tsel l√∂st, kannst du die GUI nutzen:

1.  Stelle sicher, dass ein trainiertes Modell im `output`-Ordner liegt (siehe `config.MODEL_PATH`).
2.  Starte das Spiel:
```bash
python sudoku_game.py
```
3.  W√§hle den Schwierigkeitsgrad √ºber den **"Level"-Slider** aus.
4.  Klicke auf **"New Game"** und dann auf **"Solve"**, um den Agenten beim L√∂sen zuzusehen.

---

## üìä Visualisierung des Trainings
Wenn in der `config.py` der Parameter `RENDER_GUI = True` gesetzt ist, √∂ffnet das Training f√ºr jeden Agenten ein eigenes Fenster. So kannst du live beobachten, wie die KI verschiedene Strategien ausprobiert.

---
*Entwickelt als Experimentierfeld f√ºr Reinforcement Learning in komplexen Constraint-Umgebungen.*
