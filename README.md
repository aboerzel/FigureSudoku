# üß© FigureSudoku RL-Agent

Dieses Projekt demonstriert den Einsatz von **Reinforcement Learning** (Best√§rkendes Lernen), um eine komplexe Sudoku-Variante zu l√∂sen. Anstelle von Zahlen verwendet dieses Sudoku geometrische **Formen** und **Farben**, was die logischen Anforderungen an den Agenten erh√∂ht.

---

## üé® Das Spielkonzept

Das **FigureSudoku** basiert auf einem 4x4-Gitter. Jedes Feld muss eine eindeutige Kombination aus einer Form und einer Farbe enthalten.

### Die Attribute:
*   **Geometrien:** üîµ Kreis, üü• Quadrat, ‚ñ≤ Dreieck, ‚¨¢ Hexagon
*   **Farben:** ‚ù§Ô∏è Rot, üíö Gr√ºn, üíô Blau, üíõ Gelb

### Die Regeln:
1.  Jedes Feld muss eine Figur (Form + Farbe) enthalten.
2.  In jeder **Reihe** und jeder **Spalte** darf jede Form nur einmal vorkommen.
3.  In jeder **Reihe** und jeder **Spalte** darf jede Farbe nur einmal vorkommen.
4.  Jede Kombination (z.B. "Roter Kreis") darf im gesamten Gitter nur einmal existieren.
5.  **Teilvorgaben:** Es ist m√∂glich, dass Felder nur mit einer Form (ohne Farbe) oder nur mit einer Farbe (ohne Form) vorbelegt sind. Der Agent muss dann die jeweils fehlende Komponente logisch korrekt erg√§nzen.

---

## üöÄ Die KI-Architektur

Der Agent nutzt modernste Deep-Learning-Techniken, um die Spielregeln von Grund auf zu lernen:

*   **Algorithmus:** `MaskablePPO` (Proximal Policy Optimization). Dank **Action Masking** lernt der Agent keine ung√ºltigen Z√ºge, was das Training massiv beschleunigt.
*   **CNN (Convolutional Neural Network) mit Residual Blocks (ResNet):** Da Sudoku-Regeln auf r√§umlichen Abh√§ngigkeiten (Zeilen/Spalten) basieren, nutzt der Agent Faltungsschichten. ResNet-Bl√∂cke helfen dabei, auch tieferliegende Abh√§ngigkeiten ohne Informationsverlust zu lernen.
*   **Observation Space:** Ein 3D-Tensor (10 Kan√§le), der One-Hot-kodiert die Positionen aller Formen und Farben repr√§sentiert (flattened auf 160 Eing√§nge).
*   **Action Space:** Insgesamt 256 diskrete Aktionen. Jede Aktion entspricht der Kombination aus einer bestimmten Figur (16 M√∂glichkeiten) und einem Zielfeld (16 Felder).
*   **Action Masking:** Da in jedem Zustand nur wenige der 256 Aktionen regelkonform sind, werden ung√ºltige Z√ºge (z.B. doppelte Farbe in einer Reihe) maskiert. Der Agent w√§hlt nur aus den verbleibenden validen Optionen.
*   **Curriculum Learning:** Das Training startet bei Level 1 (fast gel√∂st) und steigert automatisch den Schwierigkeitsgrad bis Level 12 (viele leere Felder), sobald der Agent eine definierte Erfolgsquote (einstellbar √ºber `REWARD_THRESHOLD`) erreicht.
*   **Fortsetzbarkeit:** Das Training erkennt automatisch vorhandene Modelle und setzt das Curriculum-Level basierend auf dem letzten Log-Eintrag in der in `config.py` definierten Log-Datei (`LOG_FILE_PATH`) fort.
*   **Backtracking-Generator:** Die R√§tsel werden mithilfe eines Backtracking-Algorithmus generiert, der sicherstellt, dass die Aufgaben l√∂sbar sind und optional eine eindeutige L√∂sung besitzen.

---

## üß† Funktionsweise des Agenten

Der L√∂sungsprozess folgt einem klassischen RL-Zyklus:

1.  **Beobachtung:** Der Agent sieht das aktuelle 4x4-Gitter als One-Hot-Vektor.
2.  **Maskierung:** Die Umgebung berechnet alle regelkonformen Z√ºge basierend auf den Sudoku-Regeln.
3.  **Entscheidung:** Das neuronale Netz bewertet die validen Aktionen und w√§hlt die Erfolgversprechendste aus.
4.  **Belohnung:** F√ºr jeden korrekten Zug erh√§lt der Agent einen kleinen Reward. Das L√∂sen des gesamten R√§tsels gibt einen gro√üen Bonus.
5.  **Lernen:** √úber PPO optimiert der Agent seine Strategie, um die kumulierte Belohnung zu maximieren.

---

## üìÇ Projektstruktur

```text
FigureSudoku/
‚îú‚îÄ‚îÄ üìÑ config.py             # Zentrale Konfiguration (Hyperparameter, Level, etc.)
‚îú‚îÄ‚îÄ üìÑ train.py              # Hauptskript zum Starten des KI-Trainings
‚îú‚îÄ‚îÄ üìÑ figure_sudoko_env.py  # Die Gymnasium-Umgebung (Logik & Rewards)
‚îú‚îÄ‚îÄ üìÑ sudoku_generator.py   # Backtracking-Algorithmus zur R√§tsel-Generierung (mit optionaler Eindeutigkeitspr√ºfung)
‚îú‚îÄ‚îÄ üìÑ sudoku_game.py        # Grafische Oberfl√§che zum Spielen & Evaluieren
‚îú‚îÄ‚îÄ üìÑ visualizer.py         # Live-Visualisierung w√§hrend des Trainings
‚îú‚îÄ‚îÄ üìÑ callbacks.py          # Logik f√ºr Curriculum Learning & Modell-Speicherung
‚îú‚îÄ‚îÄ üìÑ shapes.py             # Definitionen der Formen und Farben (Enums)
‚îî‚îÄ‚îÄ üìÅ output/               # Gespeicherte Modelle, Logs und Checkpoints
```

---

## ‚öôÔ∏è Konfiguration (`config.py`)

Die zentralen Einstellungen des Projekts werden in der `config.py` vorgenommen. Hier eine √úbersicht der wichtigsten Parameter:

### üß© Generator (R√§tsel-Erstellung)
*   `START_LEVEL`: Level, bei dem das Training beginnt (Anzahl leere Felder bzw. Felder ohne vollst√§ndige Figur). [Bereich: `1` bis `16`]
*   `MAX_LEVEL`: Das Ziel-Level (h√∂chste Schwierigkeit). [Bereich: `1` bis `16`, aktuell `12`]
*   `UNIQUE`: Stellt sicher, dass jedes generierte R√§tsel nur genau eine g√ºltige L√∂sung hat. [Werte: `True`, `False`]
*   `PARTIAL_PROB`: Wahrscheinlichkeit (`0.0` bis `1.0`), dass in einem R√§tsel Teilvorgaben (nur Farbe oder nur Form) generiert werden. Erh√∂ht die Komplexit√§t, da der Agent fehlende Attribute erg√§nzen muss.
*   `PARTIAL_MODE`: Bestimmt die Anzahl der Teilvorgaben pro R√§tsel:
    *   `0`: Deaktiviert.
    *   `1`: Genau 2 Felder werden als Teilvorgaben markiert.
    *   `2`: Zuf√§llig 1 bis 2 Felder werden als Teilvorgaben markiert.

### ‚ö° Training & Hyperparameter
*   `NUM_AGENTS`: Anzahl der parallelen Trainings-Umgebungen. [Bereich: `>= 1`]
*   `REWARD_THRESHOLD`: Die ben√∂tigte Erfolgsquote (z.B. `0.90` f√ºr 90%), um in das n√§chste Level aufzusteigen. [Bereich: `0.0` bis `1.0`]
*   `CHECK_FREQ`: Intervall (in Schritten), in dem die Erfolgsquote gepr√ºft und Modelle zwischengespeichert werden. [Bereich: `>= 1`]
*   `TOTAL_TIMESTEPS`: Die Gesamtdauer des Trainings (Gesamtzahl der Schritte √ºber alle Agenten). [Bereich: `>= 1`]
*   `MAX_TIMESTEPS`: Maximale Anzahl an Schritten pro Episode. Verhindert Endlosschleifen bei unl√∂sbaren Zust√§nden.

### üèÜ Belohnungssystem (Rewards)
*   `REWARD_SOLVED`: Belohnung f√ºr ein komplett gel√∂stes Sudoku. [Typ: `Float`, empfohlen: `> 0`]
*   `REWARD_VALID_MOVE_BASE`: Basisbelohnung f√ºr einen korrekten Setzvorgang. Die tats√§chliche Belohnung ist dynamisch und wird mit der Anzahl der leeren Felder skaliert: `base * (1 + empty_fields / state_size)`. Dies f√∂rdert gezielte Z√ºge auf einem leeren Board.
*   `REWARD_INVALID_MOVE`: Strafe f√ºr den Versuch, eine Figur entgegen der Regeln zu platzieren. [Typ: `Float`, empfohlen: `< 0`]

### üñºÔ∏è Visualisierung
*   `RENDER_GUI`: Aktiviert die Live-Anzeige der Agenten w√§hrend des Trainings. [Werte: `True`, `False`]

---

## üõ† Setup & Installation

### Voraussetzungen:
*   Python 3.8+
*   Anaconda oder venv (empfohlen)

### Installation der Abh√§ngigkeiten:
```bash
pip install -r requirements.txt
```

---

## üèãÔ∏è Training starten

Um den Agenten zu trainieren, f√ºhre einfach die `train.py` aus. Die Konfiguration kann in der `config.py` angepasst werden (z.B. `NUM_AGENTS` f√ºr Parallelisierung).

```bash
python train.py | Tee-Object -FilePath output/SUDOKU/training.log
```

### Monitoring mit TensorBoard:
W√§hrend das Training l√§uft, kannst du den Fortschritt (Erfolgsquote, Reward) live verfolgen. Der Pfad ist in `config.TENSORBOARD_TRAIN_LOG` definiert:
```bash
# Beispiel (standardm√§√üig):
tensorboard --logdir output/SUDOKU/logs/train --port 6006
```
√ñffne dann `http://localhost:6006` in deinem Browser.

---

## üéÆ Den Agenten beobachten (Test/Demo)

Wenn du sehen m√∂chtest, wie die trainierte KI ein R√§tsel l√∂st, kannst du die GUI nutzen:

1.  Stelle sicher, dass ein trainiertes Modell im `output`-Ordner liegt (siehe `config.MODEL_PATH`).
2.  Starte das Spiel:
```bash
python sudoku_game.py
```
3.  W√§hle den Schwierigkeitsgrad √ºber den **"Level"-Slider** aus.
4.  Klicke auf **"New Game"** und dann auf **"Solve"**, um den Agenten beim L√∂sen zuzusehen.

---

## üìä Visualisierung des Trainings
Wenn in der `config.py` der Parameter `RENDER_GUI = True` gesetzt ist, √∂ffnet das Training f√ºr jeden Agenten ein eigenes Fenster. So kannst du live beobachten, wie die KI verschiedene Strategien ausprobiert.

---
*Entwickelt als Experimentierfeld f√ºr Reinforcement Learning in komplexen Constraint-Umgebungen.*
