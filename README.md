# üß© Figure-Sudoku RL-Agent

Dieses Projekt demonstriert den Einsatz von **Reinforcement Learning** (Best√§rkendes Lernen), um eine komplexe Sudoku-Variante zu l√∂sen. Anstelle von Zahlen verwendet dieses Sudoku geometrische **Formen** und **Farben**, was die logischen Anforderungen an den Agenten erh√∂ht.

---

## üé® Das Spielkonzept

Das **Figure-Sudoku** basiert auf einem 4x4-Gitter. Jedes Feld muss eine eindeutige Kombination aus einer Form und einer Farbe enthalten.

### Die Attribute:
*   **Geometrien:** üîµ Kreis, üü• Quadrat, ‚ñ≤ Dreieck, ‚¨¢ Hexagon
*   **Farben:** ‚ù§Ô∏è Rot, üíö Gr√ºn, üíô Blau, üíõ Gelb

### Die Regeln:
1.  Jedes Feld muss eine Figur (Form + Farbe) enthalten.
2.  In jeder **Reihe** und jeder **Spalte** darf jede Form nur einmal vorkommen.
3.  In jeder **Reihe** und jeder **Spalte** darf jede Farbe nur einmal vorkommen.
4.  Jede Kombination (z.B. "Roter Kreis") darf im gesamten Gitter nur einmal existieren.
5.  **Teilvorgaben:** Es ist m√∂glich, dass Felder nur mit einer Form (ohne Farbe) oder nur mit einer Farbe (ohne Form) vorbelegt sind. Der Agent muss dann die jeweils fehlende Komponente logisch korrekt erg√§nzen.

---

## üöÄ Die KI-Architektur

Der Agent nutzt modernste Deep-Learning-Techniken, um die Spielregeln von Grund auf zu lernen:

*   **Bibliotheken:** Nutzt **Stable Baselines3 (v2.0+)** und **Gymnasium**, die aktuelle Standard-Schnittstelle f√ºr Reinforcement Learning.
*   **Algorithmus:** `MaskablePPO` (Proximal Policy Optimization). Dank **Action Masking** lernt der Agent keine ung√ºltigen Z√ºge, was das Training massiv beschleunigt.
*   **CNN (Convolutional Neural Network) mit Residual Blocks (ResNet):** Da Sudoku-Regeln auf r√§umlichen Abh√§ngigkeiten (Zeilen/Spalten) basieren, nutzt der Agent Faltungsschichten. ResNet-Bl√∂cke helfen dabei, auch tieferliegende Abh√§ngigkeiten ohne Informationsverlust zu lernen.
*   **Observation Space:** Ein 3D-Tensor (10 Kan√§le), der One-Hot-kodiert die Positionen aller Formen und Farben repr√§sentiert (flattened auf 160 Eing√§nge).
*   **Action Space:** Insgesamt 256 diskrete Aktionen. Jede Aktion entspricht der Kombination aus einer bestimmten Figur (16 M√∂glichkeiten) und einem Zielfeld (16 Felder).
*   **Action Masking:** Da in jedem Zustand nur wenige der 256 Aktionen regelkonform sind, nutzt das Projekt **Action Masking**. Dies verhindert, dass der Agent ung√ºltige Z√ºge (z.B. doppelte Farbe in einer Reihe) √ºberhaupt in Erw√§gung zieht. Dies reduziert den Suchraum dramatisch und stabilisiert das Training (siehe Abschnitt [Action Masking](#-action-masking-detailerkl√§rung)).
*   **Curriculum Learning:** Das Training startet bei Level 1 (fast gel√∂st) und steigert automatisch den Schwierigkeitsgrad bis Level 12 (viele leere Felder), sobald der Agent eine definierte Erfolgsquote (einstellbar √ºber `REWARD_THRESHOLD`) erreicht.
*   **Fortsetzbarkeit:** Das Training erkennt automatisch vorhandene Modelle. Das Start-Level wird prim√§r √ºber `START_LEVEL` in der `config.py` gesteuert. Ist dieser Wert auf `None` gesetzt, wird das Level automatisch aus dem letzten Log-Eintrag (`LOG_FILE_PATH`) ermittelt (mit Fallback auf Level 1).
*   **R√§tsel-Generator:** Die R√§tsel werden mithilfe eines hochoptimierten Backtracking-Algorithmus generiert (`sudoku_generator.py`). Dieser nutzt die **HCDS-Metrik** (Human-Centric Difficulty System), um gezielt Schwierigkeitsgrade von Level 1 bis 12 zu erzeugen, die das menschliche Schwierigkeitsempfinden abbilden. Er stellt sicher, dass jede Aufgabe eine eindeutige L√∂sung besitzt. Ab Level 11 werden zudem Teilvorgaben (Partial Shapes) unterst√ºtzt.

---

## üß† Funktionsweise des Agenten

Der L√∂sungsprozess folgt einem klassischen RL-Zyklus:

1.  **Beobachtung:** Der Agent sieht das aktuelle 4x4-Gitter als One-Hot-Vektor.
2.  **Maskierung:** Die Umgebung berechnet alle regelkonformen Z√ºge basierend auf den Sudoku-Regeln.
3.  **Entscheidung:** Das neuronale Netz bewertet die validen Aktionen und w√§hlt die Erfolgversprechendste aus.
4.  **Belohnung:** F√ºr jeden korrekten Zug erh√§lt der Agent einen kleinen Reward. Das L√∂sen des gesamten R√§tsels gibt einen gro√üen Bonus.
5.  **Lernen:** √úber PPO optimiert der Agent seine Strategie, um die kumulierte Belohnung zu maximieren.

---

## üß© R√§tsel-Generator & Schwierigkeit (HCDS)

Der neue Generator (`sudoku_generator.py`) basiert auf dem **Human-Centric Difficulty System (HCDS)**. Er wurde speziell optimiert, um das menschliche Schwierigkeitsempfinden in den Level-Stufen 1-12 abzubilden und auch bei hohen Schwierigkeitsgraden eine schnelle R√§tsel-Generierung (ca. 3s f√ºr Level 12) bei garantierter Eindeutigkeit zu erm√∂glichen.

### Schwierigkeitsstufen (1-12):
*   **Level 1-10:** Die Schwierigkeit skaliert linear durch das Entfernen von Feldern, bis der Ziel-HCDS-Wert erreicht ist.
*   **Level 11:** Enth√§lt zus√§tzlich **eine Teilvorgabe** (nur Form oder nur Farbe).
*   **Level 12:** Enth√§lt **zwei Teilvorgaben**.

### Performance-Features:
*   **MRV-Heuristik (Minimum Remaining Values):** Beschleunigt die Eindeutigkeitspr√ºfung durch intelligente Wahl des n√§chsten Feldes im Backtracking.
*   **In-Place Backtracking:** Minimiert Speicherallokationen und CPU-Last.
*   **Inkrementelle HCDS-Berechnung:** Effiziente Bewertung der Schwierigkeit w√§hrend des Generierungsprozesses.

---

## üõ°Ô∏è Action Masking (Detailerkl√§rung)

Action Masking ist eine entscheidende Technik f√ºr die Effizienz dieses Agenten. Da der Action Space mit **256 Aktionen** sehr gro√ü ist, aber in jedem Spielzustand oft nur **weniger als 5%** der Z√ºge legal sind, w√ºrde ein Standard-RL-Agent extrem lange brauchen, um allein die Grundregeln (z.B. "nicht zweimal Rot in eine Spalte") durch reines Ausprobieren (*Trial & Error*) zu lernen.

### Wie es funktioniert:
Bevor der Agent eine Aktion ausw√§hlt, berechnet die Umgebung (`FigureSudokuEnv.action_masks()`) einen bin√§ren Vektor (die Maske). F√ºr jede der 256 Aktionen wird gepr√ºft:

1.  **Feldbelegung:** Ist das Zielfeld bereits mit einer anderen Figur belegt? (Oder passt die gew√§hlte Figur zu einer bestehenden Teilvorgabe?)
2.  **Figur-Verf√ºgbarkeit:** Wurde die Kombination aus Form und Farbe (z.B. "Blauer Kreis") bereits an einer anderen Stelle im Gitter platziert?
3.  **Sudoku-Constraints (Reihe/Spalte):** Existiert die gew√§hlte Form oder die gew√§hlte Farbe bereits in der Ziel-Reihe oder Ziel-Spalte?

### Warum MaskablePPO?
In einem Standard-PPO-Algorithmus w√ºrde der Agent auch ung√ºltige Aktionen w√§hlen, eine negative Belohnung erhalten und dann m√ºhsam lernen, diese Aktionen zu vermeiden.
**MaskablePPO** hingegen nutzt die Maske direkt in der Wahrscheinlichkeitsverteilung der Policy:
*   Ung√ºltige Aktionen erhalten eine Wahrscheinlichkeit von **exakt Null**.
*   Der Agent "sieht" w√§hrend der Entscheidungsfindung nur die legalen Optionen.
*   **Vorteil:** Das neuronale Netz muss keine Kapazit√§t darauf verschwenden, die harten Regeln des Spiels auswendig zu lernen, sondern kann sich sofort auf die **L√∂sungsstrategie** konzentrieren.

---

## üìÇ Projektstruktur

```text
FigureSudoku/
‚îú‚îÄ‚îÄ üìÑ config.py             # Zentrale Konfiguration (Hyperparameter, Level, etc.)
‚îú‚îÄ‚îÄ üìÑ train.py              # Hauptskript zum Starten des KI-Trainings
‚îú‚îÄ‚îÄ üìÑ figure_sudoku_env.py  # Die Gymnasium-Umgebung (Logik & Rewards)
‚îú‚îÄ‚îÄ üìÑ sudoku_generator.py   # Hochoptimierter Generator mit HCDS-Metrik & Eindeutigkeitspr√ºfung
‚îú‚îÄ‚îÄ üìÑ sudoku_game.py        # Grafische Desktop-Oberfl√§che (Tkinter)
‚îú‚îÄ‚îÄ üìÑ streamlit_app.py      # Moderne Web-Applikation (Streamlit)
‚îú‚îÄ‚îÄ üìÑ visualizer.py         # Live-Visualisierung w√§hrend des Trainings
‚îú‚îÄ‚îÄ üìÑ callbacks.py          # Logik f√ºr Curriculum Learning & Modell-Speicherung
‚îú‚îÄ‚îÄ üìÑ shapes.py             # Definitionen der Formen und Farben (Enums)
‚îî‚îÄ‚îÄ üìÅ output/               # Gespeicherte Modelle, Logs und Checkpoints
```

---

## ‚öôÔ∏è Konfiguration (`config.py`)

Die zentralen Einstellungen des Projekts werden in der `config.py` vorgenommen. Hier eine √úbersicht der wichtigsten Parameter:

### üß© Generator (R√§tsel-Erstellung)
*   `START_LEVEL`: Bestimmt das Start-Level f√ºr das Training. Wenn ein Wert (1-12) angegeben ist, wird dieser fest verwendet (manuelles √úberschreiben). Ist `None` gesetzt, wird das Level beim Fortsetzen eines Trainings automatisch aus der Log-Datei ermittelt (Fallback: Level 1). [Bereich: `1` bis `12` oder `None`]
*   `MAX_LEVEL`: Das Ziel-Level (h√∂chste Schwierigkeit). [Bereich: `1` bis `12`]

### ‚ö° Training & Hyperparameter
*   `NUM_AGENTS`: Anzahl der parallelen Trainings-Umgebungen. [Bereich: `>= 1`]
*   `REWARD_THRESHOLD`: Die ben√∂tigte Erfolgsquote (z.B. `0.90` f√ºr 90%), um in das n√§chste Level aufzusteigen. [Bereich: `0.0` bis `1.0`]
*   `CHECK_FREQ`: Intervall (in Schritten), in dem die Erfolgsquote gepr√ºft und Modelle zwischengespeichert werden. [Bereich: `>= 1`]
*   `TOTAL_TIMESTEPS`: Die Gesamtdauer des Trainings (Gesamtzahl der Schritte √ºber alle Agenten). [Bereich: `>= 1`]
*   `MAX_TIMESTEPS`: Maximale Anzahl an Schritten pro Episode. Verhindert Endlosschleifen bei unl√∂sbaren Zust√§nden.

### üèÜ Belohnungssystem (Rewards)

Das Belohnungssystem ist darauf ausgelegt, den Agenten zu einem effizienten und regelkonformen L√∂sungsweg zu f√ºhren. Es besteht aus drei Hauptkomponenten:

1.  **`REWARD_SOLVED` (Aktuell: `10.0`)**:
    *   **Zweck:** Der "Heilige Gral". Dies ist die maximale Belohnung, die der Agent erh√§lt, wenn das gesamte Gitter regelkonform gef√ºllt ist.
    *   **Warum dieser Wert?** Er muss deutlich h√∂her sein als die Summe der Einzelz√ºge, damit der Agent das √ºbergeordnete Ziel (das L√∂sen) priorisiert. Selbst auf dem h√∂chsten Schwierigkeitsgrad (Level 12) betr√§gt die Summe aller validen Einzelzug-Belohnungen nur ca. `2.45`, was bedeutet, dass der `REWARD_SOLVED` (10.0) immer noch mehr als das Vierfache davon wert ist. Dies stellt sicher, dass der Agent auch bei komplexen R√§tseln stets motiviert bleibt, das R√§tsel vollst√§ndig zu l√∂sen.

2.  **`REWARD_VALID_MOVE_BASE` (Aktuell: `0.1`)**:
    *   **Zweck:** Belohnung f√ºr jeden korrekten Zug.
    *   **Dynamische Skalierung:** Die tats√§chliche Belohnung berechnet sich als: `base * (1 + leere_felder / gitter_gr√∂√üe)`.
    *   **Warum diese Logik?** Durch die Skalierung erh√§lt der Agent f√ºr Z√ºge auf einem leeren Board (wo es viele M√∂glichkeiten gibt) eine h√∂here Belohnung als f√ºr Z√ºge auf einem fast vollen Board. Dies motiviert den Agenten, "schwierige" Entscheidungen fr√ºhzeitig korrekt zu treffen. Der Basiswert von `0.1` ist klein genug, um "Reward Shaping" zu erm√∂glichen, ohne das Endziel zu √ºberschatten.

3.  **`REWARD_INVALID_MOVE` (Aktuell: `-0.5`)**:
    *   **Zweck:** Bestrafung f√ºr illegale Z√ºge (obwohl diese durch Action Masking weitgehend verhindert werden).
    *   **Warum dieser Wert?** Die Strafe ist moderat negativ gew√§hlt. Da der Agent `MaskablePPO` nutzt, trifft er selten auf ung√ºltige Z√ºge im Action Space, aber die Strafe dient als zus√§tzliche Absicherung f√ºr die Lernstabilit√§t der Policy.

---

## üñºÔ∏è Visualisierung (Training)

W√§hrend des Trainings kann der Fortschritt auf zwei Arten visualisiert werden:

*   **Live-GUI:** Wenn `config.RENDER_GUI = True` gesetzt ist, wird der Spielzustand der Agenten live in einem Fenster (`visualizer.py`) angezeigt.
*   **TensorBoard:** Detaillierte Metriken (Reward, Erfolgsquote, Training-Loss) werden geloggt.

---

## üõ† Setup & Installation

### Voraussetzungen:
*   Python 3.8+
*   Anaconda oder venv (empfohlen)
*   CUDA-f√§hige GPU (f√ºr Training empfohlen, z.B. CUDA 11.8)

### Installation der Abh√§ngigkeiten:

1.  **PyTorch mit CUDA-Support (Beispiel f√ºr CUDA 11.8):**
    ```bash
    pip install torch==2.3.1+cu118 torchvision==0.18.1+cu118 torchaudio==2.3.1+cu118 --extra-index-url https://download.pytorch.org/whl/cu118
    ```

2.  **Restliche Anforderungen:**
    ```bash
    pip install -r requirements.txt
    ```

---

## üèãÔ∏è Training starten

Um den Agenten zu trainieren, f√ºhre einfach die `train.py` aus. Die Konfiguration kann in der `config.py` angepasst werden (z.B. `NUM_AGENTS` f√ºr Parallelisierung).

```bash
python train.py | Tee-Object -FilePath output/SUDOKU/training.log
```

### Monitoring mit TensorBoard:
W√§hrend das Training l√§uft, kannst du den Fortschritt (Erfolgsquote, Reward) live verfolgen. Der Pfad ist in `config.TENSORBOARD_TRAIN_LOG` definiert:
```bash
# Beispiel (standardm√§√üig):
tensorboard --logdir output/SUDOKU/logs/train --port 6006
```
√ñffne dann `http://localhost:6006` in deinem Browser.

---

## üéÆ Spielen & Den Agenten beobachten

Es stehen zwei Oberfl√§chen zur Verf√ºgung, um das Spiel selbst zu spielen oder die KI beim L√∂sen zu beobachten.

### üåê Web-Applikation (Streamlit) - Empfohlen
Eine moderne, interaktive Weboberfl√§che, die im Browser l√§uft.
```bash
streamlit run streamlit_app.py
```

### üñ•Ô∏è Desktop-Anwendung (Tkinter)
Die klassische Version mit Drag & Drop Funktionalit√§t.
```bash
python sudoku_game.py
```

### Anleitung:
1.  Stelle sicher, dass ein trainiertes Modell im `output`-Ordner liegt (siehe `config.MODEL_PATH`).
2.  W√§hle den Schwierigkeitsgrad √ºber den **"Level"-Slider** aus.
3.  Klicke auf **"Neues Spiel"** (oder generiere ein neues R√§tsel).
4.  Klicke auf **"L√∂sen"**, um den Agenten beim L√∂sen zuzusehen, oder spiele selbst!

---

## üìä Visualisierung des Trainings (Live)
Wenn in der `config.py` der Parameter `RENDER_GUI = True` gesetzt ist, √∂ffnet das Training f√ºr jeden Agenten ein eigenes Fenster (`visualizer.py`). So kannst du live beobachten, wie die KI verschiedene Strategien ausprobiert.

---

## üìÑ Lizenz & Autor

*   **Autor:** Andreas B√∂rzel
*   **GitHub:** [Figure-Sudoku](https://github.com/aboerzel/FigureSudoku)
*   **Lizenz:** [MIT License](LICENSE) (oder siehe Dateikopf)

*Entwickelt als Experimentierfeld f√ºr Reinforcement Learning in komplexen Constraint-Umgebungen.*
